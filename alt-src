#!/usr/bin/python

import ConfigParser
import cStringIO
import datetime
import fcntl
import hashlib
import logging
import koji
from optparse import OptionParser
import os
import os.path
import re
import rpm
import shutil
import smtplib
import subprocess
import sys
import tempfile
import traceback



'''
Given an srpm and product, stage for alt-src release
'''


def _(args):
    """Stub function for translation"""
    return args


def get_options():
    """process options from command line"""

    usage = _("%prog [options] branch srpm")
    parser = OptionParser(usage=usage)
    parser.add_option("-c", "--config", dest="cfile", default='/etc/altsrc.conf',
                      help=_("use alternate configuration file"), metavar="FILE")
    parser.add_option("-v", "--verbose", action="store_true", default=False,
                      help=_("be more verbose"))
    parser.add_option("-q", "--quiet", action="store_true", default=False,
                      help=_("be less verbose"))
    parser.add_option("-d", "--debug", action="store_true", default=False,
                      help=_("show debug output"))
    parser.add_option("--restage", action="store_true", default=False,
                      help=_("remove and recreate staged content"))
    parser.add_option("--push", action="store_true", default=False,
                      help=_("push staged sources"))
    parser.add_option("--brew", action="store_true", default=False,
                      help=_("pull sources from Brew"))
    parser.add_option("-o", "--option", dest="copts", action="append", metavar="OPT=VALUE",
                      help=_("set config option"))
    (options, args) = parser.parse_args()

    options.branch = args[0]
    options.source = args[1]

    options.config = get_config(options.cfile, options.copts)

    return options


config_defaults = {
    'stagedir' : '/srv/cache/stage',
    'gitdir' : '/srv/git',
    'rulesdir' : '/var/lib/altsrc/rules',
    'lookaside' : '/srv/cache/lookaside',
    'git_push_url' : '/srv/git/pushtest/%(package)s.git',
    'git_fetch_url' : None,  # None means same as push
    'lookaside_rsync_dest' : '/srv/cache/lookaside2',
    'init_rsync_dest' : '/srv/git/pushtest/%(package)s.git',
    'log_level' : 'INFO',
    'log_file' : None,
    'log_format' : '%(asctime)s [%(levelname)s] %(message)s',
    'whitelist' : '',
    'blacklist' : '',
    'changelog_user' : 'CentOS Sources <bugs@centos.org>',
    'git_name' : 'CentOS Sources',
    'git_email' : 'bugs@centos.org',
    'koji_hub' : 'http://brewhub.devel.redhat.com/brewhub',
    'koji_topdir' : '/mnt/redhat/brewroot',
    'smtp_enabled' : True,
    'smtp_host' : 'localhost',
    #'smtp_host' : 'smtp.corp.redhat.com',
    'smtp_from' : 'Alt Source Stager <altsrc@redhat.com>',
    'smtp_to' : 'mike',
    'smtp_log_to' : '',
}

config_int_opts = set([])
config_bool_opts = set(['smtp_enabled'])

def get_config(cfile, overrides):
    if not os.access(cfile, os.F_OK):
        die("Missing config file: %s" % cfile)
    cp = ConfigParser.RawConfigParser()
    cp.read(cfile)
    if not cp.has_section('altsrc'):
        die("Configuration file missing [altsrc] section: %s" % cfile)

    #apply overrides from command line
    overrides = overrides or []
    for opt in overrides:
        parts = opt.split("=", 1)
        if len(parts) != 2:
            die('Invalid option specification: %s\nUse OPT=VALUE' % opt)
        key, value = parts
        cp.set('altsrc', key, value)

    #generate config dictionary
    config = dict(config_defaults)  #copy
    for key in cp.options('altsrc'):
        if key in config_int_opts:
            config[key] = cp.getint('altsrc', key)
        elif key in config_bool_opts:
            config[key] = cp.getboolean('altsrc', key)
        else:
            config[key] = cp.get('altsrc', key)

    #sanity checks
    if not os.path.isdir(config['stagedir']):
        die("No such directory: %s" % config['stagedir'])

    return config



class SpecFile(object):
    '''
        Spec manipulation functions
        Based on similar class from python-rpmpatch
        https://cdcvs.fnal.gov/redmine/projects/python-rpmpatch/
    '''

    def __init__(self, specfile, changelog_user):
        '''
            Read in the specfile and setup our environment
        '''
        self.logger = logging.getLogger("altsrc")
        self.specfile = specfile

        self.changelog_user = changelog_user
        self.changelog = {}

        self.changelog_done = False

        self.text = None

        _fd = open(self.specfile, 'r')
        self.text = _fd.read()
        _fd.close()

        basedir = os.path.dirname(self.specfile)
        self.basedir = basedir
        self.sourcesdir = os.path.abspath(basedir + '/../SOURCES/')
        if os.path.isfile(self.sourcesdir + '/' + self.specfile):
            os.remove(self.sourcesdir + '/' + self.specfile)
        #shutil.copy2(self.specfile, self.sourcesdir)

    def __del__(self):
        '''
            Make sure to save changes
        '''
        if self.text != None:
            self.save()

    def run_re(self, regex_match, regex_replace, changelog):
        '''
            Run a regex against the specfile
        '''
        self.text = re.sub(regex_match, regex_replace, self.text)

        regname = regex_match + ' => ' + regex_replace

        #self.changelog['Ran Regex: ' + regname] = changelog

        return True

    def apply_specfile_diff(self, spec_patch, changelog):
        '''
            For any actual changes to the specfile you will need to generate
            a nice simple unified diff summarizing your changes.  The diff will
            be automatically added to the SRPM as a 'SOURCE' file so that you
            can easily review your changes in the future.

            This is not pure python. http://bugs.python.org/issue2057

            Your patch must have a stripe of '0' or things wont work right.
            Rather than accounting for all use cases I'm just forcing a simple
            one so you will have to deal with it.
        '''
        tempdir = tempfile.mkdtemp()
        specname = os.path.basename(self.specfile)
        _fd = open(tempdir + '/' + specname, 'w')
        _fd.write(self.text)
        _fd.close()

        _fd = open(spec_patch, 'r')

        thisdir = os.getcwd()
        os.chdir(tempdir)
        code = subprocess.call(['patch' , '-p0'], stdin=_fd,
                               stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                               shell=True)
        _fd.close()
        if code != 0:
            raise RuntimeError(code, "Spec diff failed")

        _fd = open(tempdir + '/' + specname, 'r')
        self.text = _fd.read()
        _fd.close()

        self.add_source(spec_patch, None, changelog=changelog)

        os.chdir(thisdir)
        shutil.rmtree(tempdir)

        return True

    def add_patch(self, patchfile, patchstripe, patchnum, changelog):
        '''
            This will add a given patch into the rpm package.  You must
            specify the patch stripe (the 1 from -p1).

            A unique number is required for the patchnum.
            You can specify one or set it to 'None' and a somewhat random,
            though unique, number will be created for you.

            You must provide a changelog reason for this modification.
        '''
        patch_re = '([pP]atch(\d*):\s+(\S+))'
        matches = re.findall(patch_re, self.text)
        # if we are making a number up
        if patchnum == None:
            # if there are no existing patches we start at 0, else at 1
            if not matches:
                patchnum_list = [ 0 ]
            else:
                patchnum_list = [ 1 ]

            # put all the numbers in a list
            for match in matches:
                if match[1]:
                    patchnum_list.append(int(match[1]))

        # find the biggest num and + 1
        patchnum = (max(patchnum_list) + 1)

        patchname = os.path.basename(patchfile)
        if os.path.isfile(self.basedir + '/../SOURCES/' + patchname):
            os.remove(self.basedir + '/../SOURCES/' + patchname)
        self.logger.debug("Copy %s -> %s", patchfile, self.sourcesdir)
        shutil.copy2(patchfile, self.sourcesdir)
        entry = "\nPatch" + str(patchnum) + ":\t" + patchname

        matches = re.findall(patch_re, self.text)
        if matches:
            patch_entry = matches[-1][0]
        else:
            namematch = re.findall('(\s*[nN]ame:\s+.*)', self.text)
            patch_entry = namematch[0]

        self.text = self.text.replace(patch_entry, patch_entry + entry)

        entry = "\n%patch" + str(patchnum) + ' -p' + str(patchstripe)
        patch_re = '(%patch\d+\s+-p\d.*)'
        matches = re.findall(patch_re, self.text)
        if matches:
            patch_entry = matches[-1]
        else:
            setup_re = '(%setup.*)'
            matches = re.findall(setup_re, self.text)
            patch_entry = matches[0]

        self.text = self.text.replace(patch_entry, patch_entry + entry)

        #self.changelog['Added Patch: ' + patchname] = changelog

        return (patchnum, patchname)

    def rm_patch(self, patchname, patchnum, changelog):
        '''
            Prevent a given patch from being applied to the rpm package.

            You can describe the patch by name or by number, but you
            must pick one!  Set the other to 'None'

            You must provide a changelog reason for this modification.
        '''

        if patchname != None:
            patch_re = '(\s*[pP]atch(\d+):\s+(' + patchname + ')\n)'
        elif patchnum != None:
            patch_re = r'(\s*[pP]atch(' + str(patchnum) + r'):\s+(\S+?)\s+?)'
        else:
            raise ValueError('You must specify something I can use here')

        matches = re.findall(patch_re, self.text)
        if not matches:
            self.logger.error("Spec does not match pattern: %r", patch_re)
            raise SanityError, "Could not find patch to remove"
        self.text = self.text.replace(matches[0][0], '\n')

        patchnum = matches[0][1]
        patchname = matches[0][2]

        # also remove the line that applies the patch
        patch_re = '(%patch' + patchnum + '.+?\n)'
        match_patch = re.findall(patch_re, self.text)
        if match_patch:
            self.text = self.text.replace(match_patch[0], '\n')
        else:
            # possibly this is because autosetup is doing it
            prog = re.compile(r'^\s*%autosetup.* -S git\b', re.MULTILINE)
            if prog.search(self.text):
                # TODO add a flag to indicate this is expected
                self.logger.warning('Patch %s appears to be applied by %%autosetup', patchname)
            else:
                self.logger.error('No %%patch line for patch %s', patchname)
                raise SanityError, "Unable to remove patch"

        #self.changelog['Removed Patch: ' + patchname] = changelog

        return (patchnum, patchname)

    def add_source(self, sourcefile, sourcenum, changelog):
        '''
            Add a source file to the rpm spec.

            A unique number is required for the sourcenum.
            You can specify one or set it to 'None' and a somewhat random,
            though unique, number will be created for you.

            You must provide a changelog reason for this modification.
        '''
        source_re = '([sS]ource(\d*):\s+(\S+))'
        matches = re.findall(source_re, self.text)
        # if we are making a number up
        if sourcenum == None:
            # if there are no existing sources we start at 0, else at 1
            if not matches:
                sourcenum_list = [ 0 ]
            else:
                sourcenum_list = [ 1 ]

            # put all the numbers in a list
            for match in matches:
                if match[1]:
                    sourcenum_list.append(int(match[1]))

        # find the biggest num and + 1
        sourcenum = (max(sourcenum_list) + 1)

        sourcename = os.path.basename(sourcefile)
        if os.path.isfile(self.basedir + '/../SOURCES/' + sourcename):
            os.remove(self.basedir + '/../SOURCES/' + sourcename)
        self.logger.debug("Copy %s -> %s", sourcefile, self.sourcesdir)
        shutil.copy2(sourcefile, self.sourcesdir)
        entry = "\nSource" + str(sourcenum) + ":\t" + sourcename

        matches = re.findall(source_re, self.text)
        if matches:
            source_entry = matches[-1][0]
        else:
            namematch = re.findall('(\s*[nN]ame:\s+.+)', self.text)
            source_entry = namematch[0]

        self.text = self.text.replace(source_entry, source_entry + entry)

        #self.changelog['Added Source: ' + sourcename] = changelog

        return (sourcenum, sourcename)

    def __add_changelog(self):
        '''
            Record the changes in the changelog
        '''
        if self.changelog_done == True:
            return
        now = datetime.datetime.now().strftime('%a %b %d %Y ')
        changelog_text = '%changelog\n* ' + now + self.changelog_user + '\n'
        for action in self.changelog.keys():
            changelog_text = changelog_text + '- ' + action + '\n'
            changelog_text = changelog_text + '-->  ' + self.changelog[action]
            changelog_text = changelog_text + '\n'

        changelog_text = changelog_text + '\n'

        self.text = self.text.replace('%changelog\n', changelog_text)

        self.changelog_done = True

        return True

    def save(self):
        '''
            Write out the specfile with changes
        '''
        #self.__add_changelog()
        _fd = open(self.specfile, 'w')
        _fd.write(self.text)
        _fd.close()

        return True


class StageError(Exception):
    """Our base error class"""
    pass

class StartupError(StageError):
    """Raised when an error happens very early"""
    pass

class CommandError(StageError):
    """Raised when a command we run fails"""

class FilterError(StageError):
    pass

class SanityError(StageError):
    pass

class ConfigError(StageError):
    pass



class BaseProcessor(object):

    def __init__(self, options):
        self.options = options
        self.logger = logging.getLogger("altsrc")
        self.error_log = None
        if options.brew:
            self.srpm = self.get_koji_srpm(options.source)
        else:
            self.srpm = options.source
        if not os.path.isfile(self.srpm):
            raise StartupError, "No such file: %s" % self.srpm
        self.logfile = None
        self.workdir = None

    def run(self):
        raise NotImplementedError

    def add_error_logger(self):
        """Capture all error messages for use in later notifications"""
        self.error_log = cStringIO.StringIO()
        handler = logging.StreamHandler(self.error_log)
        handler.setFormatter(logging.Formatter(self.options.config['log_format']))
        handler.setLevel(logging.ERROR)
        self.error_handler = handler
        self.logger.addHandler(handler)

    def remove_error_logger(self):
        self.logger.removeHandler(self.error_handler)

    def log_cmd(self, cmd, logfile=None, fatal=True, **kwargs):
        """Run command and log output if able"""
        self.logger.info('Running command: %s', ' '.join(cmd))
        if logfile:
            kwargs.setdefault('stdout', self.logfile)
            kwargs.setdefault('stderr', subprocess.STDOUT)
            kwargs.setdefault('close_fds', True)
        elif self.logfile:
            self.logfile.flush()
            kwargs.setdefault('stdout', self.logfile)
            kwargs.setdefault('stderr', subprocess.STDOUT)
            kwargs.setdefault('close_fds', True)
        proc = subprocess.Popen(cmd, **kwargs)
        ret = proc.wait()
        if ret:
            if fatal:
                raise CommandError, "command failed: %r" % cmd
            #otherwise
            self.logger.warn("Command failed: %r" % cmd)
        return ret

    def get_output(self, cmd, fatal=True, **kwargs):
        """Run command and log output if able"""
        self.logger.info('Getting output from command: %s', ' '.join(cmd))
        kwargs['stdout'] = subprocess.PIPE
        kwargs.setdefault('close_fds', True)
        if 'stderr' in kwargs:
            # convenience values
            if kwargs['stderr'] == 'null':
                kwargs['stderr'] = file('/dev/null', 'w')
            elif kwargs['stderr'] == 'keep':
                kwargs['stderr'] = subprocess.STDOUT
        elif self.logfile:
            self.logfile.flush()
            kwargs['stderr'] = self.logfile
        proc = subprocess.Popen(cmd, **kwargs)
        output = proc.communicate()[0]
        self.logger.debug("Command output was:\n%s", output)
        rv = proc.wait()
        if rv:
            self.logger.warn("Command failed: %r" % cmd)
            if fatal:
                raise CommandError, "command failed: %r" % cmd
        return output, rv

    def get_koji_srpm(self, source):
        """Assume source is an n-v-r.a from koji and get the file from there"""
        session = koji.ClientSession(self.options.config['koji_hub'], {'anon_retry':True})
        pathinfo = koji.PathInfo(self.options.config['koji_topdir'])
        rpminfo = session.getRPM(source)
        if not rpminfo:
            raise SanityError, "No such rpm: %s" % source
        if rpminfo['arch'] != 'src':
            raise SanityError, "Not a source rpm: %s" % source
        binfo = session.getBuild(rpminfo['build_id'])
        bdir = pathinfo.build(binfo)
        relpath = pathinfo.rpm(rpminfo)
        return os.path.join(bdir, relpath)

    def read_srpm(self):
        self.logger.info('Reading source rpm: %s', self.srpm)
        h = koji.get_rpm_header(self.srpm)
        self.headers = h
        if h[rpm.RPMTAG_SOURCEPACKAGE] != 1:
            raise InputError, "%s is not a source package" % self.srpm
        data = koji.get_header_fields(h, ['name','version','release','summary'])
        self.nvr = "%(name)s-%(version)s-%(release)s" % data
        self.package = data['name']
        self.version = data['version']
        self.summary = data['summary']

    def check_package(self):
        """Check whitelist/blacklist and load any package rules"""

        whitelist = self.options.config['whitelist'].split()
        if whitelist:
            whitelisted = koji.util.multi_fnmatch(self.package, whitelist)
        else:
            whitelisted = False
        if not whitelisted:
            blacklist = self.options.config['blacklist'].split()
            if blacklist and koji.util.multi_fnmatch(self.package, blacklist):
                raise FilterError, 'Blacklisted package: %s' % self.package

    def git_push_url(self):
        params = {'package': self.package }  #XXX anything else?
        git_url =  self.options.config['git_push_url'] % params
        return git_url

    def git_fetch_url(self):
        url_form = (self.options.config['git_fetch_url']
                    or self.options.config['git_push_url'])
        params = {'package': self.package }  #XXX anything else?
        git_url =  url_form % params
        return git_url

    def git_base_cmd(self):
        cmd = ['git',
                '-c', 'user.name=%s' % self.options.config['git_name'],
                '-c', 'user.email=%s' % self.options.config['git_email'],
        ]
        return cmd

    def get_workdir(self):
        letter = self.package[0]
        if not letter.isalpha():
            letter = "_"
        parts = [
            os.path.join(self.options.config['stagedir']),
            self.options.branch,
            letter,
            self.package,
            self.nvr,
        ]
        # branch included in the path because debranding rules can depend on it
        return os.path.join(*parts)

    def find_spec(self, relative=False):
        """Locate specfile in checkout"""
        specdir = os.path.join(self.checkout, 'SPECS')

        #first look for $package.spec
        path = os.path.join(specdir, self.package + '.spec')
        if os.path.isfile(path):
            return path

        #otherwise
        for fn in os.listdir(specdir):
            if fn.endswith('.spec'):
                return os.path.join(specdir, fn)

        raise SanityError, 'No spec file in checkout: %s' % self.checkout


    def duplicate_check(self):
        """Check to see if we're already on remote"""
        tagname = self.get_import_tagname()
        git_url =  self.git_fetch_url()
        self.logger.debug("Checking remote %s for tag %s", git_url, tagname)
        self.logger.debug("Logfile is %r", self.logfile)
        cmd = ['git', 'ls-remote', git_url, "refs/tags/%s" % tagname]
        output, rv = self.get_output(cmd, stderr='null', fatal=False)
        if rv:
            # ignore for now. either repo is missing (ok) or network/server broken (we'll die later)
            self.logger.warning("Unable to check remote repo: %s", git_url)
            return False
        if output:
            self.logger.warning("Tag %s already present on remote", tagname)
            return True
            #TODO further verification

    def get_state(self):
        if not self.workdir:
            return None
        statefile = os.path.join(self.workdir, 'state')
        self.logger.debug('Reading state file: %s', statefile)
        fo = open(statefile, 'r')
        try:
            fcntl.lockf(fo.fileno(), fcntl.LOCK_SH)
            state = fo.read()
            self.logger.debug('Read state: %s', state)
        finally:
            fo.close()
        return state.strip()

    def set_state(self, state):
        if not self.workdir:
            raise SanityError, "No workdir to set state for"
        statefile = os.path.join(self.workdir, 'state')
        self.logger.debug('Writing state %s to file %s', state, statefile)
        fo = open(statefile, 'w')
        try:
            fcntl.lockf(fo.fileno(), fcntl.LOCK_EX)
            fo.write(state)
        finally:
            fo.close()
        return state

    def sanitize_ref(self, ref):
        return '/'.join([self.sanitize_ref_segment(seg) for seg in ref.split('/')])

    git_sanitize_re = ( r'((?:^[.]'
                        r'|[.]$'
                        r'|[[:cntrl:]]'
                        r'|[ %~^:?*/\\[])+)' )
                        # % is ok in refs, but we use it to escape the rest

    def sanitize_ref_segment(self, ref):
        """Sanitize a segment of a git reference

        Hex encodes any problematic character sequences
        See the git-check-ref-format man page for reference naming rules
        """
        prog = re.compile(self.git_sanitize_re)
        parts = []
        ofs = 0
        for match in prog.finditer(ref):
            a, b = match.span()
            parts.append(ref[ofs:a])
            p2 = ref[a:b]
            p2 = ''.join(['%%%2x' % ord(c) for c in p2])
            parts.append(p2)
            ofs = b
        parts.append(ref[ofs:]) #tail
        ref = ''.join(parts)
        # deal with multichar patterns
        ref = re.sub(r'[.][.]', '.%2e', ref)
        ref = re.sub(r'[.]lock$', '%2elock', ref)
        ref = re.sub(r'@{', '%40{', ref)
        return ref

    def get_import_tagname(self):
        safe_nvr = self.sanitize_ref_segment(self.nvr)
        tagname = "imports/%s/%s" % (self.options.branch, safe_nvr)
        return tagname

    def send_email_notice(self, level, subject, body, extra_headers=None):
        do_send = True
        if not self.options.config['smtp_enabled']:
            self.logger.warning("SMTP disabled. Skipping email notification.")
            do_send = False
            #return
        if level == 'info':
            recipients = self.options.config['smtp_log_to']
        else:
            recipients = self.options.config['smtp_to']
        if not recipients:
            self.logger.warning("No recipients configured. Skipping email notification.")
            do_send = False
            #return
        package = self.package or "UNKNOWN"
        headers = [
            ('From', self.options.config['smtp_from']),
            ('Subject', subject),
            ('To', recipients),
            ('X-altsrc-package', package),
        ]
        if extra_headers:
            headers.extend(extra_headers)

        head = "\n".join(["%s: %s" % (k,v) for k,v in headers])
        message = "%s\n\n%s" % (head, body)
        message.replace('\n', '\r\n')
        message = koji.fixEncoding(message)
        self.logger.debug("Email notice contents:\n%s\n", message)
        if not do_send:
            self.logger.debug("Email not sent")
            return
        server = smtplib.SMTP(self.options.config['smtp_host'])
        server.sendmail(self.options.config['smtp_from'], self.options.config['smtp_to'], message)
        server.quit()

    def notify_errors(self):
        """Send notification about non-fatal errors"""

        if not self.error_log:
            return
        error_messages = self.error_log.getvalue()
        if not error_messages:
            return
        self.logger.warning("Sending notification of non-fatal errors")
        nvr = self.nvr or "UNKNOWN"
        subject = 'Non-fatal errors for %s' % nvr
        body = "Errors:\n%s" % error_messages
        self.error_log.seek(0)
        self.error_log.truncate()
        #TODO : be more informative
        self.send_email_notice('error', subject, body)

    def handle_error(self):
        """Handle an exception. Called from top level."""
        tb = ''.join(traceback.format_exception(*sys.exc_info()))
        self.logger.exception('Staging failed')
        cls, err = sys.exc_info()[:2]
        msg = ''.join(traceback.format_exception_only(cls, err))
        self.logger.warning("Sending error email")
        nvr = self.nvr or "UNKNOWN"
        subject = 'Error staging %s' % nvr
        body = """\
Staging failed for %(nvr)s.

%(tb)s
"""
        body = body % locals()
        #TODO : be more informative
        if self.error_log:
            error_messages = self.error_log.getvalue()
            if error_messages:
                body = "%s\nErrors:\n%s\n" % (body, error_messages)
                self.error_log.seek(0)
                self.error_log.truncate()

        self.send_email_notice('error', subject, body)
        if self.logfile:
            logfilename = getattr(self.logfile, 'name')
            if logfilename:
                self.logger.info("Logfile: %s", logfilename)


class Stager(BaseProcessor):

    def run(self):
        self.add_error_logger()
        self.read_srpm()
        self.check_package()
        if self.duplicate_check():
            self.logger.warning('Skipping staging for duplicate content')
            return
        if self.make_workdir() in ['STAGED', 'PUSHED']:
            return
        self.setup_logfile()
        self.sync_repo()
        self.setup_checkout()
        self.import_srpm()
        try:
            self.debrand()
        except (SystemExit, KeyboardInterrupt):
            raise
        except Exception:
            self.logger.exception('Debranding failed')
            self.handle_debrand_fail()
        self.set_state("STAGED")
        self.notify()
        self.remove_error_logger()

    def make_workdir(self):
        self.workdir = dirname = self.get_workdir()
        if os.path.islink(dirname):
            raise SanityError, "%s is a symlink" % dirname
        elif os.path.isdir(dirname):
            # TODO - more sanity checks
            if self.options.restage:
                self.logger.warn("Overwriting existing workdir: %s",  dirname)
                # TODO - back up first
                koji.util.rmtree(dirname)
            else:
                state = self.get_state()
                if state == 'STAGED':
                    self.logger.warn("Already successfully staged: %s",  dirname)
                    return state
                elif state == 'PUSHED':
                    self.logger.warn("Already successfully pushed: %s",  dirname)
                    return state
                raise SanityError, "Incomplete staging dir %s (state=%s), use --restage to overwrite." \
                        % (dirname, state)
        elif os.path.exists(dirname):
            raise SanityError, "%s exists and is not a directory" % dirname
        self.logger.info('Creating working directory: %s', dirname)
        koji.ensuredir(dirname)
        return self.set_state("INIT")

    def setup_logfile(self):
        self.logfile = file(os.path.join(self.workdir, 'stage.log'), 'w')
        handler = logging.StreamHandler(self.logfile)
        handler.setFormatter(logging.Formatter(self.options.config['log_format']))
        handler.setLevel(self.options.file_log_level)
        self.logger.addHandler(handler)


    def sync_repo(self):
        """Sync the primary (bare) git repo from the (local) master"""

        repo = os.path.join(self.options.config['gitdir'], "%s.git" % self.package)

        if not os.path.exists(repo):
            self.init_repo()
            return

        # if our local copy exists, we assume that remote does as well

        git_url =  self.git_fetch_url()
        self.logger.info('Syncing primary repo: %s', repo)
        cmd = ['git', 'fetch', '-v', git_url, '+refs/*:refs/*']
        self.log_cmd(cmd, cwd=repo)

        # TODO - add sanity checks


    def init_repo(self):
        """Initialize a new repo"""
        # check if repo exists on remote
        git_url =  self.git_fetch_url()
        self.logger.info("Checking if remote repo exists: %s", git_url)
        cmd = ['git', 'ls-remote', git_url, "refs/heads/master"]
        output, rv = self.get_output(cmd, fatal=False)
        if rv:
            # error talking to remote
            # for now, we assume this means the remote does not exist
            #TODO distinguish this from other errors (e.g. network)
            self.logger.warning("Remote repo is missing: %s", git_url)
            self.init_new_repo()
            return
        # otherwise we just need to clone it
        cmd = ['git', 'clone', '--bare', git_url, "%s.git" % self.package]
        self.log_cmd(cmd, cwd=(self.options.config['gitdir']))

    gitblit_config_format = r'''
[gitblit]
        description = %(summary)s
        owner = kbsingh
        useTickets = false
        useDocs = false
        accessRestriction = PUSH
        showRemoteBranches = false
        isFrozen = false
        showReadme = true
        skipSizeCalculation = false
        skipSummaryMetrics = false
        federationStrategy = FEDERATE_THIS
        isFederated = false
'''

    def init_new_repo(self):
        initdir = os.path.join(self.workdir, "repo_init")
        self.logger.info('Initializing new repo: %s', initdir)
        koji.ensuredir(initdir)
        cmd = ['git', 'init']
        self.log_cmd(cmd, cwd=initdir)
        readme = os.path.join(initdir, 'README.md')
        fo = open(readme, 'w')
        # XXX this text need to live elsewhere
        fo.write('''\
The master branch has no content

Look at the c7 branch if you are working with CentOS-7, or the c4/c5/c6 branch for CentOS-4, 5 or 6
If you find this file in a distro specific branch, it means that no content has been checked in yet
''')
        fo.close()
        cmd = ['git', 'add', 'README.md']
        self.log_cmd(cmd, cwd=initdir)
        cmd = self.git_base_cmd()
        cmd.extend(['commit', '-m', 'init git for %s' % self.package])
        self.log_cmd(cmd, cwd=initdir)
        branches = self.options.config.get('init_branches')
        if branches:
            for distbranch in branches:
                cmd = ['git', 'branch', distbranch]
                self.log_cmd(cmd, cwd=initdir)
        #finally create a bare repo from our working copy
        cmd = ['git', 'clone', '--bare', initdir, "repo_init.git"]
        self.log_cmd(cmd, cwd=self.workdir)
        descfile = os.path.join(self.workdir, "repo_init.git", "description")
        fo = file(descfile, 'w')
        fo.write(self.summary)
        fo.write('\n')
        fo.close()
        # add gitblit options to git config
        # XXX this content should not be hard coded
        git_config = os.path.join(self.workdir, "repo_init.git", "config")
        fo = file(git_config, 'a')
        params = {
            'summary' : self.summary,
            'package' : self.package,
            # anything else?
        }
        fo.write(self.gitblit_config_format % params)

    def setup_checkout(self):
        """Setup our working checkout"""

        src = os.path.join(self.options.config['gitdir'], "%s.git" % self.package)
        if not os.path.exists(src):
            # should be new repo case
            src = os.path.join(self.workdir, "repo_init.git")
        dst = os.path.join(self.workdir, "checkout")
        self.checkout = dst
        self.logger.info('Setting up working checkout: %s', dst)
        cmd = ['git', 'clone', '--local', '-v', src, dst]
        self.log_cmd(cmd, cwd=self.workdir)

        #create and checkout our staging branch
        branchname = "altsrc-stage-%s" % self.options.branch
        branchname = self.sanitize_ref_segment(branchname)
        self.branchname = branchname
        base = 'refs/remotes/origin/%s' % self.options.branch
        self.logger.info('Setting up staging branch: %s', branchname)
        cmd = ['git', 'show-ref', '--verify', base]
        rv = self.log_cmd(cmd, cwd=dst, fatal=False)
        if rv:
            self.logger.warn('Base branch %s missing, staging on an orphan branch', self.options.branch)
            cmd = ['git', 'checkout', '--orphan', branchname]
            self.log_cmd(cmd, cwd=dst)
            cmd = ['git', 'rm', '-rf', '--ignore-unmatch', '.']
            self.log_cmd(cmd, cwd=dst)
        else:
            cmd = ['git', 'checkout', '-b', branchname, base]
            self.log_cmd(cmd, cwd=dst)

        self.set_state("CHECKOUT")


    # file extensions that are automatically placed in lookaside
    # (should all be lower case)
    UPLOAD_EXTS = [
        'tar', 'gz', 'bz2', 'lzma', 'xz', 'z', 'zip', 'tff',
        'bin', 'tbz', 'tbz2', 'tgz', 'tlz', 'txz', 'pdf', 'rpm',
        'jar', 'war', 'db', 'cpio', 'jisp', 'egg', 'gem', 'iso',
        ]

    # file extensions that are automatically included in repo
    # (should all be lower case)
    INCLUDE_EXTS = [
        'spec', 'patch', 'diff',
        'html', 'txt', 'init', 'conf', 'sh',
        ]

    def for_lookaside(self, path):
        """Determine if a file should go to the lookaside cache

        Return True if file should go to the lookaside
        """
        # there are varying heuristics for what to place in the lookaside
        # Fedora/pyrpkg decides based on extension
        # Centos/nazar decides based on output of the file utility
        # We're using a hybrid approach
        ext = path.rsplit('.')[-1].lower()
        if ext in self.INCLUDE_EXTS:
            return False
        if ext in self.UPLOAD_EXTS:
            return True
        st = os.stat(path)
        if st.st_size < 1024:
            # the UPLOAD_EXTS check should catch most of what we want in
            # the lookaside, so we'll just include anything small
            return False
        #lastly, see what the file utility says
        cmd = ['file', '--brief', path]
        # XXX should we use --mime instead?
        output, rv = self.get_output(cmd, stderr='keep', fatal=False)
        if rv:
            #nonfatal
            return False
        self.logger.debug("Source file %s: %s", path, output)
        return output.find('text') == -1

    def import_srpm(self):
        """Import our srpm on the specified branch"""

        #explode our srpm
        dst = self.checkout
        wipe_git_dir(dst)
        self.logfile.flush()
        explode_srpm(self.srpm, dst, logfile=self.logfile)

        #figure out which sources go to the lookaside
        to_move = []
        sourcedir = os.path.join(dst, 'SOURCES')
        for fn in os.listdir(sourcedir):
            path = os.path.join(sourcedir, fn)
            if self.for_lookaside(path):
                to_move.append(fn)
        to_move.sort()

        # move files to lookaside
        meta = file(os.path.join(dst, ".%s.metadata" % self.package), 'w')
        gitignore = file(os.path.join(dst, ".gitignore"), 'w')
        for fn in to_move:
            path = os.path.join(sourcedir, fn)
            csum = hashlib.sha1()
            # XXX should we move to something stronger?
            fo = file(path, 'rb')
            chunk = 'IGNORE ME!'
            while chunk:
                chunk = fo.read(8192)
                csum.update(chunk)
            fo.close()
            digest = csum.hexdigest()
            dirname = os.path.join(self.options.config['lookaside'], self.package, self.options.branch)
            # XXX - using Centos/nazar path for now, may need to change
            lpath = os.path.join(dirname, digest)
            if not os.path.isfile(lpath):
                koji.ensuredir(dirname)
                self.logger.info('Copying source file to lookaside: %s -> %s', fn, lpath)
                shutil.copy2(path, lpath)
            else:
                # we appear to already have it
                st1 = os.stat(path)
                st2 = os.stat(lpath)
                if st1.st_size != st2.st_size:
                    self.logger.error("Possibly corrupt lookaside entry: %s", lpath)
                    self.logger.error("Size: %s, but current matching source is %s", st1.st_size, st2.st_size)
                    raise SanityError, "Lookaside size mismatch"
                # TODO - more sanity checks
                self.logger.info('Skipping source, already in digest: %s', fn)
                #os.unlink(path)
            # write metadata file
            meta.write("%s SOURCES/%s\n" % (digest, fn))
            # write .gitignore
            gitignore.write('SOURCES/%s\n' % fn)
        meta.close()
        gitignore.close()

        cmd = ['git', 'add', '-A', '.']
        self.log_cmd(cmd, cwd=dst)

        # see if we have anything to commit
        cmd = ['git', 'diff', '--cached', '--name-only']
        output, rv = self.get_output(cmd, cwd=dst, stderr='keep', fatal=False)
        if not output:
            raise SanityError, "Nothing to commit. Already pushed?"
        # if we're called again for some reason the commit will fail

        cmd = self.git_base_cmd()
        cmd.extend(['commit', '-m', 'import %s' % self.nvr])
        self.log_cmd(cmd, cwd=dst)
        self.set_state("IMPORT")

        #tag this import for reference
        #this is not the tag we will actually push because of the rebase
        tagname = "altsrc-stage-import-%s" % self.options.branch
        tagname = self.sanitize_ref_segment(tagname)
        cmd = ['git', 'tag', tagname]
        self.log_cmd(cmd, cwd=self.checkout)

    def debrand(self):
        """Apply debranding rules"""
        #search for applicable rules

        cp = ConfigParser.RawConfigParser()
        for name in 'altsrc-global', self.package:
            cfile = os.path.join(self.options.config['rulesdir'], name + '.cfg')
            self.logger.debug('Looking for rules in %s', cfile)
            if not os.access(cfile, os.F_OK):
                continue
            self.logger.info('Loading rules from %s', cfile)
            cp.read(cfile)

        # order rules
        rules = []
        for section in cp.sections():
            parts = section.split(None, 1)
            if len(parts) < 2:
                continue
            rtype, key = parts
            rules.append((key, rtype, section))
        rules.sort()

        if not rules:
            self.logger.info('No debranding rules found')
            return

        # apply rules
        changelog_notes=[]
        for key, rtype, section in rules:
            handler = 'rule_handler_%s' % rtype
            if not hasattr(self, handler):
                raise ConfigError, "No handler for rule type %s" % rtype
            data = dict(cp.items(section))
            clog_note = data.get('changelog')
            if clog_note:
                changelog_notes.append(clog_note)
            if 'enabled' in data:
                enabled = data['enabled'].lower().strip()
                if enabled in ('no', 'false', '0'):
                    self.logger.info('Skipping disabled rule: %s', section)
                    continue
            if 'on_package' in data:
                patterns = data['on_package'].split()
                if not koji.util.multi_fnmatch(self.package, patterns):
                    self.logger.debug('Skipping rule due to package filter: %s', section)
                    continue
            if 'on_version' in data:
                patterns = data['on_version'].split()
                if not koji.util.multi_fnmatch(self.version, patterns):
                    self.logger.debug('Skipping rule due to version filter: %s', section)
                    continue
            if 'on_branch' in data:
                patterns = data['on_branch'].split()
                if not koji.util.multi_fnmatch(self.options.branch, patterns):
                    self.logger.debug('Skipping rule due to branch filter: %s', section)
                    continue
            self.logger.info("Applying rule: %s", section)
            can_fail = False
            if data.get('can_fail', '').lower() in ('yes', 'y', '1', 'true'):
                can_fail = True
            try:
                getattr(self, handler)(data)
                # TODO - check result
            except (SystemExit, KeyboardInterrupt):
                raise
            except Exception:
                self.logger.exception('Failed to apply debranding rule (%s: %s) to %s',
                        rtype, key, self.nvr)
                if not can_fail:
                    # caller will handle
                    raise

        # and commit changes
        cmd = ['git', 'add', '-A', '.']
        self.log_cmd(cmd, cwd=self.checkout)

        # write out changelog for later
        # we'll update the date and add to spec at push time
        self.prep_changelog(changelog_notes)

        # check that we actually have something to commit
        cmd = ['git', 'diff', '--cached', '--name-only']
        output, rv = self.get_output(cmd, cwd=self.checkout, stderr='keep', fatal=False)
        if not output:
            raise SanityError, "Debranding rules made no changes"
            # caller will clean up

        cmd = self.git_base_cmd()
        cmd.extend(['commit', '-m', 'debranding changes'])
        self.log_cmd(cmd, cwd=self.checkout)
        self.set_state("DEBRAND")

    def handle_debrand_fail(self):
        """If debranding fails, clean up and note failure"""

        self.logger.warning("Resetting checkout to import")
        tagname = "altsrc-stage-import-%s" % self.options.branch
        tagname = self.sanitize_ref_segment(tagname)
        cmd = ['git', 'reset', '--hard', "refs/tags/%s" % tagname]
        self.log_cmd(cmd, cwd=self.checkout)

        self.logger.warning("Adding debranding failure notice")
        fn = os.path.join(self.checkout, "README.debrand")
        fo = file(fn, 'w')
        fo.write('''\
Warning: This package was configured for automatic debranding, but the changes
failed to apply.
''')
        fo.close()
        cmd = ['git', 'add', 'README.debrand']
        self.log_cmd(cmd, cwd=self.checkout)
        #and commit
        cmd = ['git', 'commit', '-m', 'Debranding failure notice']
        self.log_cmd(cmd, cwd=self.checkout)

    def prep_changelog(self, notes):
        """Generate changelog info"""
        self.logger.info('Preparing changelog entry')
        user = self.options.config['changelog_user']
        verrel = self.get_modded_verrel()
        #now = datetime.datetime.now().strftime('%a %b %d %Y')
        parts = ['* INSERT_DATE_HERE %s - %s\n' % (user, verrel),
                 '- Apply debranding changes\n']
        for note in notes:
            lineno=0
            for line in note.splitlines():
                line = line.strip()
                if line:
                    lineno += 1
                    if lineno == 1:
                        parts.append('- %s\n' % line)
                    else:
                        parts.append('-  %s\n' % line)
        fo = file(os.path.join(self.workdir, 'changelog.txt'), 'w')
        for part in parts:
            fo.write(part)
            self.logger.debug("%s", part)
        fo.close()

    def get_modded_verrel(self):
        spec = self.find_spec()
        macros = self.get_mod_macros()
        cmd = ['rpm', '-q', '--specfile', spec]
        for m in macros:
            cmd.extend(['--define', '%s %s' % (m, macros[m])])
        cmd.extend([
               '--qf', '%{v}-%{r}\\n'])
        output, rv = self.get_output(cmd, cwd=self.checkout)
        verrel = output.splitlines()[0]
        return verrel

    def get_mod_macros(self):
        #TODO
        return {
            "dist" : ".el7.centos", #XXX
            "rhel" : "7", #XXX
            }

    def rule_handler_spec(self, data):
        """Patch the spec file"""

        spec = SpecFile(self.find_spec(), self.options.config['changelog_user'])

        changelog = data.get('changelog')
        fn = data['patch']
        patch = os.path.join(self.options.config['rulesdir'], fn)
        spec.apply_specfile_diff(patch, changelog)

    def rule_handler_re(self, data):
        """Apply a regex substitution to the spec file"""

        spec = SpecFile(self.find_spec(), self.options.config['changelog_user'])

        changelog = data.get('changelog')
        spec.run_re(data['match'], data['replace'], changelog)

    def rule_handler_patch(self, data):
        """Add or remove a patch in the spec file"""

        spec = SpecFile(self.find_spec(), self.options.config['changelog_user'])

        method = data['method'].lower()
        changelog = data.get('changelog')
        num = data.get('num')
        if method == 'add':
            name = data['patch']
            name = os.path.join(self.options.config['rulesdir'], name)
            stripe = data.get('stripe', 1)
            spec.add_patch(name, stripe, num, changelog)

        elif method == 'del':
            if 'patch' in data:
                name = os.path.basename(data['patch'])
            else:
                name = None

            # XXX I guess this is a common rpmpatch config typo?
            if num == name:
                raise ConfigError, 'Bad patch section in config'

            return spec.rm_patch(name, num, changelog)


    def rule_handler_source(self, data):
        """Add a source to the specfile"""

        spec = SpecFile(self.find_spec(), self.options.config['changelog_user'])

        changelog = data.get('changelog')
        method = data['method'].lower()
        num = data.get('num')
        if method == 'add':
            thisfile = data['source']
            spec.add_source(thisfile, num, changelog)

        #XXX - no other methods??
        else:
            raise ConfigError, 'Bad source section in config file'

    def rule_handler_script(self, data):
        """Run an arbitary script"""
        fn = data['script']
        script = os.path.join(self.options.config['rulesdir'], fn)
        if not os.path.isfile(script):
            raise ConfigError, 'Script missing: %s' % script

        cmd = [script, self.checkout, self.find_spec()]
        self.log_cmd(cmd, cwd=self.checkout)

    def remake_srpm(self):
        """Remake the srpm"""
        # This is mainly a test
        cmd = ['rpmbuild', '-bs',
                '--define', '_topdir %s' % self.checkout,
                '--define', '_srcrpmdir %s' % self.workdir,
                '--define', 'dist .TEST',
                self.find_spec()]
        self.log_cmd(cmd, cwd=self.workdir)


    def notify(self):
        subject = 'Successfully staged %s' % self.nvr
        body = """\
Staging completed for %(nvr)s.

Working directory: %(workdir)s
""" % vars(self)
        body = body % locals()
        #TODO : be more informative
        self.send_email_notice('info', subject, body)


class Pusher(BaseProcessor):

    def run(self):
        self.add_error_logger()
        self.read_srpm()
        self.check_package()
        if self.duplicate_check():
            self.logger.warning('Skipping push for duplicate content')
            return
        if self.check_workdir() == 'PUSHED':
            return
        self.setup_logfile()
        self.add_changelog()
        self.push_lookaside()
        self.push_git()
        self.set_state('PUSHED')
        self.notify()
        self.remove_error_logger()


    def check_workdir(self):
        self.workdir = dirname = self.get_workdir()
        self.logger.info('Checking working directory: %s', dirname)
        if os.path.islink(dirname):
            raise SanityError, "%s is a symlink" % dirname
        if not os.path.isdir(dirname):
            raise SanityError, "Not staged. No such directory: %s" % dirname
        state = self.get_state()
        if state == 'PUSHED':
            self.logger.warn('Already pushed')
            return state
        if state != 'STAGED':
            raise SanityError, "Staging incomplete"
        self.checkout = os.path.join(self.workdir, "checkout")
        return state



    def setup_logfile(self):
        self.logfile = file(os.path.join(self.workdir, 'push.log'), 'w')
        handler = logging.StreamHandler(self.logfile)
        handler.setFormatter(logging.Formatter(self.options.config['log_format']))
        handler.setLevel(self.options.file_log_level)
        self.logger.addHandler(handler)

    def add_changelog(self):
        """Check for a prepared changelog entry and add to spec if found"""

        fn = os.path.join(self.workdir, 'changelog.txt')
        if not os.path.exists(fn):
            self.logger.info("No prepared changelog found")
            return

        # make sure we're still on the stage branch
        stage_branch = "altsrc-stage-%s" % self.options.branch
        stage_branch = self.sanitize_ref_segment(stage_branch)
        self.log_cmd(['git', 'checkout', stage_branch], cwd=self.checkout)

        # get the changelog entry
        fo = file(fn, 'r')
        clog = fo.read()
        now = datetime.datetime.now().strftime('%a %b %d %Y')
        if clog.find('INSERT_DATE_HERE') == -1:
            self.logger.error("Prepared changelog is malformed")
            return
        clog = clog.replace('INSERT_DATE_HERE', now, 1)

        # insert the entry into spec
        spec = self.find_spec()
        prog = re.compile(r'^(\s*%changelog.*)$', re.MULTILINE)
        inf = file(spec, 'r')
        parts = prog.split(inf.read())
        inf.close()
        if len(parts) == 1:
            self.logger.error('Could not find changelog in spec')
            return
        elif len(parts) == 2:
            # should not be possible
            raise SanityError, 'Unable to split changelog from spec'
        outf = file(spec, 'w')
        for part in parts[:2]:
            outf.write(part)
        outf.write('\n')
        outf.write(clog)
        for part in parts[2:]:
            outf.write(part)
        if len(parts) > 3:
            self.logger.error('Found multiple %changelog macros in spec')
            # keep going
            self.logger.debug('Context:\n%s%s', parts)
        outf.close()

        # add commit
        relpath = os.path.join('SPECS', os.path.basename(spec))
        self.log_cmd(['git', 'add', relpath], cwd=self.checkout)
        self.log_cmd(['git', 'commit', '-m', 'add changelog entry'], cwd=self.checkout)

    def push_git(self):
        """Get our checkout ready for the public push"""

        # fetch from public remote
        git_url =  self.git_push_url()
        pushbranch = "altsrc-push-%s" % self.options.branch
        pushbranch = self.sanitize_ref_segment(pushbranch)
        # see if remote has it
        cmd = ['git', 'ls-remote', git_url, "refs/heads/%s" % self.options.branch]
        output, rv = self.get_output(cmd, cwd=self.checkout, fatal=False)
        if rv:
            #error talking to remote. Could be that that repo does not exist, or could be a network error
            self.init_remote_repo()
        new_branch = False
        if not output:
            # XXX we should check the output more thoroughly
            self.logger.warning("Branch missing on remote: %s", self.options.branch)
            new_branch = True
        else:
            cmd = ['git', 'fetch', '-v', git_url, "+%s:%s" % (self.options.branch, pushbranch)]
            self.log_cmd(cmd, cwd=self.checkout)

        # now apply changes on pushbranch to stage branch
        our_import_tag = "altsrc-stage-import-%s" % self.options.branch
        our_import_tag = self.sanitize_ref_segment(our_import_tag)
        stage_branch = "altsrc-stage-%s" % self.options.branch
        stage_branch = self.sanitize_ref_segment(stage_branch)
        output, rv = self.get_output(['git', 'rev-parse', our_import_tag], cwd=self.checkout, fatal=True)
        import_rev = output.strip()
        output, rv = self.get_output(['git', 'rev-parse', stage_branch], cwd=self.checkout, fatal=True)
        stage_rev = output.strip()
        #TODO assert that stage_branch is a descendant of our_import_tag
        if new_branch:
            # branch from master if new
            cmd = ['git', 'checkout', '-b', pushbranch, 'master']
        else:
            cmd = ['git', 'checkout', pushbranch]
        self.log_cmd(cmd, cwd=self.checkout)

        # set push branch to match import
        cmd = ['git', 'rm', '-rf', '--ignore-unmatch', '.']
        self.log_cmd(cmd, cwd=self.checkout)
        cmd = ['git', 'checkout', our_import_tag, '--', '.']
        self.log_cmd(cmd, cwd=self.checkout)
        # check that we actually have something to commit
        cmd = ['git', 'diff', '--cached', '--name-only']
        output, rv = self.get_output(cmd, cwd=self.checkout, stderr='keep', fatal=False)
        if not output:
            raise SanityError, "Nothing to commit. Already pushed?"

        #commit
        cmd = self.git_base_cmd()
        cmd.extend(['commit', '-m', 'import %s' % self.nvr])
        #XXX need a better commit message
        self.log_cmd(cmd, cwd=self.checkout)

        #tag
        tagname = self.get_import_tagname()
        cmd = self.git_base_cmd()
        cmd.extend(['tag', '-a', '-m', 'import %s' % self.nvr, tagname])
        self.log_cmd(cmd, cwd=self.checkout)

        if stage_rev != import_rev:
            # grab the rest of the stage branch (debranding)
            cmd = ['git', 'rm', '-rf', '--ignore-unmatch', '.']
            self.log_cmd(cmd, cwd=self.checkout)
            cmd = ['git', 'checkout', stage_branch, '--', '.']
            self.log_cmd(cmd, cwd=self.checkout)
            # check that we actually have something to commit
            cmd = ['git', 'diff', '--cached', '--name-only']
            output, rv = self.get_output(cmd, cwd=self.checkout, stderr='keep', fatal=False)
            if not output:
                raise SanityError, "Debranding commits resulted in no changes?"
            #commit
            cmd = self.git_base_cmd()
            cmd.extend(['commit', '-m', 'debrand %s' % self.nvr])
            #XXX need a better commit message
            self.log_cmd(cmd, cwd=self.checkout)
            #TODO - another tag?

        # ...and push
        cmd = ['git', 'push', git_url, "%s:%s" % (pushbranch, self.options.branch)]
        self.log_cmd(cmd, cwd=self.checkout)
        cmd = ['git', 'push', git_url, "refs/tags/%s" % tagname]
        self.log_cmd(cmd, cwd=self.checkout)

        #TODO retry loop

    def init_remote_repo(self):
        # we should have a base repo staged here
        src = os.path.join(self.workdir, "repo_init.git/")
        # note the trailing slash for rsync arg
        params = {'package': self.package }  #XXX anything else?
        dst = self.options.config['init_rsync_dest'] % params
        cmd = ['rsync', '-ivrpt', src, dst]
        self.log_cmd(cmd, cwd=self.workdir)

    def push_lookaside(self):
        meta = file(os.path.join(self.checkout, ".%s.metadata" % self.package), 'r')
        for line in meta.readlines():
            line = line.strip()
            digest, fn = line.split(None, 1)
            self.push_lookaside_file(digest)

    def push_lookaside_file(self, digest):
        relpath = os.path.join(self.package, self.options.branch, digest)
        cmd = ['rsync', '-ivpt', '--relative', relpath, self.options.config['lookaside_rsync_dest']]
        # --relative option preserves the relative path from the command line
        # this is a way to ensure we create the necessary subdirs
        self.log_cmd(cmd, cwd=self.options.config['lookaside'])


    def notify(self):
        subject = 'Successfully pushed %s' % self.nvr
        body = """\
Push completed for %(nvr)s.

Working directory: %(workdir)s
""" % vars(self)
        body = body % locals()
        #TODO : be more informative
        self.send_email_notice('info', subject, body)



def explode_srpm(srpm, destdir=None, logfile=None):
    # explode our srpm to the given directory
    h = koji.get_rpm_header(srpm)
    if h[rpm.RPMTAG_SOURCEPACKAGE] != 1:
        # we checked this earlier, but since we're about to rpm -i it,
        # let's check again
        raise SanityError, "%s is not a source package" % srpm
    if destdir is None:
        destdir = os.getcwd()
    else:
        destdir = os.path.abspath(destdir)
        koji.ensuredir(destdir)
    cmd = ['rpm', '--nosignature', '-i', '--define', '_topdir %s' % destdir, srpm]
    #print "Running: %r" % cmd
    popts = {'close_fds':True}
    if logfile:
        popts['stdout'] = logfile
        popts['stderr'] = subprocess.STDOUT
    proc = subprocess.Popen(cmd, **popts)
    ret = proc.wait()
    if ret:
        raise CommandError, "command failed: %r" % cmd


def wipe_git_dir(dirname):
    for fn in os.listdir(dirname):
        if fn == '.git':
            continue
        path = os.path.join(dirname, fn)
        if os.path.isdir(path):
            koji.util.rmtree(path)
        else:
            os.unlink(path)


def die(msg):
    print msg
    sys.exit(1)


def setup_logging(options):
    logger = logging.getLogger("altsrc")
    logger.setLevel(logging.DEBUG)

    #determine log levels
    output_log_level = logging.WARN #default
    if options.debug:
        output_log_level = logging.DEBUG
    elif options.verbose:
        output_log_level = logging.INFO
    elif options.quiet:
        output_log_level = logging.ERROR
    config_warning = None
    file_log_level = getattr(logging, options.config['log_level'], None)
    if file_log_level is None:
        # just use sane default and warn later
        file_log_level = logging.WARN
        config_warning = "Invalid log level: %s" % options.config['log_level']
    # file level should be at least as verbose as output level
    file_log_level = min(file_log_level, output_log_level)
    options.file_log_level = file_log_level

    # set up handlers
    handler = logging.StreamHandler(sys.stderr)
    handler.setFormatter(logging.Formatter(options.config['log_format']))
    handler.setLevel(output_log_level)
    logger.addHandler(handler)
    if options.config['log_file']:
        handler = logging.FileHandler(options.config['log_file'])
        handler.setFormatter(logging.Formatter(options.config['log_format']))
        handler.setLevel(file_log_level)
        logger.addHandler(handler)
    # TODO - setup koji's logger?

    if config_warning:
        logger.warning(config_warning)
    return logger


def main():
    options = get_options()
    logger = setup_logging(options)
    tasks = [Stager(options)]
    # always run stager. it's a no-op if already staged
    if options.push:
        tasks.append(Pusher(options))
    for task in tasks:
        try:
            task.run()
            task.notify_errors()
        except (SystemExit, KeyboardInterrupt), e:
            msg = ''.join(traceback.format_exception_only(*sys.exc_info()[:2]))
            logger.warn("Exiting (%s)", msg)
            sys.exit(1)
        except Exception:
            task.handle_error()
            sys.exit(2)

if __name__ == '__main__':
    main()

# the end
